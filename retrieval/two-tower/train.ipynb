{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a4fac69",
   "metadata": {},
   "source": [
    "The notebook is split into two parts:\n",
    "1. Part 1: Train a query and item towers\n",
    "2. Part 2: Generate Index table and run validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2a95942c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully:\n",
      "Dataset({\n",
      "    features: ['timestamp', 'item_id', 'is_organic', 'played_ratio_pct', 'track_length_seconds', 'uid', 'artist_id', 'album_id'],\n",
      "    num_rows: 9228\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import os\n",
    "\n",
    "filepath = '../../datasets/yambda/processed'\n",
    "train_processed = load_from_disk(os.path.join(filepath, \"train\"))\n",
    "print(\"Dataset loaded successfully:\")\n",
    "print(train_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0477a16",
   "metadata": {},
   "source": [
    "### Part 1: Build Query and Item towers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "259cd7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating samples from sequences...\n",
      "Generated 7698840 total samples.\n"
     ]
    }
   ],
   "source": [
    "import tower\n",
    "\n",
    "train_dataset = tower.MusicWindowDataset(data = train_processed, \n",
    "                                   n = 10, \n",
    "                                   global_t_max = 25395200,\n",
    "                                   max_windows_per_user=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "979ce1a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7698840, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "acdde17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared learnable embedding for unknown items: tensor([[ 0.9740,  0.1349,  0.6543, -0.8868,  0.7666,  0.9192, -0.4455,  1.4300,\n",
      "         -0.6948,  0.9295,  0.9151,  0.7456,  0.7462,  1.8621, -0.9277, -0.4953,\n",
      "          1.7496, -0.4581,  0.1979,  2.1604]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "len_unique_users = 9238\n",
    "len_unique_items = 877168\n",
    "len_unique_albums = 3367691\n",
    "len_unique_artists = 1293394\n",
    "\n",
    "user_embed_size = 14\n",
    "item_embed_size = 20\n",
    "album_embed_size = 22\n",
    "artist_embed_size = 21\n",
    "item_embed = torch.nn.Embedding(num_embeddings=len_unique_items+1, embedding_dim=item_embed_size)\n",
    "album_embed = torch.nn.Embedding(num_embeddings=len_unique_albums+1, embedding_dim=album_embed_size)\n",
    "artist_embed = torch.nn.Embedding(num_embeddings=len_unique_artists+1, embedding_dim=artist_embed_size)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # alternatively use hashes to assign \"unique\" embeddings to unseen items, albums,...\n",
    "    # give the size of the hash array, there will be some collisions. For simplicity let's just have a single embedding for now...\n",
    "    print(f\"shared learnable embedding for unknown items: {item_embed(torch.tensor([877168]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "18285a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tower import QueryTower\n",
    "\n",
    "query_input_size = user_embed_size + 2*item_embed_size + 2*artist_embed_size + 2*album_embed_size + 3\n",
    "\n",
    "# initialize and test query tower\n",
    "query_model = QueryTower(input_size = query_input_size,\n",
    "                         hidden_size = [1024, 512, 128], # Youtube paper 2019[2] section 6.3\n",
    "                         user_num_embeddings = len_unique_users+1,\n",
    "                         user_embed_size = user_embed_size,\n",
    "                         item_embed = item_embed,\n",
    "                         artist_embed = artist_embed,\n",
    "                         album_embed = album_embed,\n",
    "                         log_age_mean = 15.874020,\n",
    "                         log_age_std = 1.090574\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6385ba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tower import ItemTower\n",
    "\n",
    "# initialize and test item tower\n",
    "candidate_input_size = item_embed_size + artist_embed_size + album_embed_size + 1\n",
    "\n",
    "item_model = ItemTower(input_size = candidate_input_size,\n",
    "                       hidden_size = [1024, 512, 128],\n",
    "                       item_embed = item_embed,\n",
    "                       artist_embed = artist_embed,\n",
    "                       album_embed = album_embed\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a2c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# loop through batches and implement in_batch softmax\n",
    "\n",
    "EPOCHS = 30\n",
    "BATCH = 2048\n",
    "# BATCH = 32\n",
    "WORKERS = 8\n",
    "TAU = 0.01 # <1 sharpens predictions for logits that already had high values and helps separate pos from neg samples\n",
    "\n",
    "# dynamically estimate frequency of candidate ids in batch using weighted moving average (frequency: # of steps between seeing interaction with the same candidate ID)\n",
    "# Youtube paper 2019[2] section 4\n",
    "H = len_unique_users # large size to prevent collisions\n",
    "alpha = 0.05\n",
    "\n",
    "# reinitialize with multiple workers and bigger batch\n",
    "all_params = (\n",
    "    list(query_model.user_embed.parameters()) +\n",
    "    list(query_model.hidden_layers.parameters()) +\n",
    "    list(item_model.hidden_layers.parameters()) +\n",
    "    list(item_embed.parameters()) +\n",
    "    list(artist_embed.parameters()) +\n",
    "    list(album_embed.parameters())\n",
    ")\n",
    "optimizer = torch.optim.AdamW(\n",
    "    all_params, \n",
    "    lr=0.001\n",
    ")\n",
    "device = \"mps\"\n",
    "\n",
    "train_dataset = tower.MusicWindowDataset(data = train_processed, \n",
    "                                   n = 10, \n",
    "                                   global_t_max = 25395200)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH, shuffle=True, num_workers=WORKERS, persistent_workers=True)\n",
    "\n",
    "tower_solver = tower.TowerSolver(\n",
    "                    query_model=query_model,\n",
    "                    item_model=item_model,\n",
    "                    device = device,\n",
    "                    optimizer=optimizer,\n",
    "                    epochs = EPOCHS,\n",
    "                    batch_size = BATCH,\n",
    "                    data = train_dataloader,\n",
    "                    reset = True,\n",
    "                    H = H,\n",
    "                    alpha = alpha,\n",
    "                    tau=TAU\n",
    "                )\n",
    "tower_solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3976b8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "epochs_ix = []\n",
    "checkpoints = \"./checkpoints\"\n",
    "for f in os.listdir(checkpoints):\n",
    "    matches = re.findall(r'-?\\d+', f)\n",
    "    if len(matches):\n",
    "        epochs_ix.append(int(matches[0]))\n",
    "\n",
    "loss_history = []\n",
    "for ix in sorted(epochs_ix):\n",
    "    checkpoint = torch.load(os.path.join(checkpoints, f\"checkpoint_epoch_{ix}.pth\"))\n",
    "    # epoch = checkpoint[\"epoch\"]\n",
    "    loss_history.append(checkpoint[\"loss\"])\n",
    "\n",
    "\n",
    "# Plot the training losses.\n",
    "plt.plot(loss_history, color='blue', label='Train')\n",
    "# plt.plot(val_loss_history, color='orange', label='Val')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Tain loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e4d5f8",
   "metadata": {},
   "source": [
    "### Part 2: Build Index Table\n",
    "\n",
    "In order to quickly extract item embedding, we need to store them in a data structure that is fast, memory efficient, and accurate. The most accurate, and often used as a baseline, is knn. However, knn is terribly inefficient (O(N x D) runtime and O(N x D x 4) memory). So the techniques listed below will try to approximate knn (ANN) by trading off between speed, memory, and accuracy.\n",
    "\n",
    "IVF - Kmeans clustering\n",
    "- build: trains a kmeans algorithm to find nlist amount of centroids that represent centers of the clusters\n",
    "- inference: given a query embedding, find nprobe amount of centroids closests to the query embedding, then search exhaustively for the closest item embeddings within the candidate clusters.\n",
    "- Better speed, worse accuracy, slighly worse memory (D x 4 bytes per vector vs D x 4 + 8 bytes per vector)\n",
    "\n",
    "PQ - Compresses D into M dimension.\n",
    "- build: splits item vector into M subspaces, trains kmeans for each subspace separately, uses cluster ID of the closest centroid in each subspace to represent the original embedding.\n",
    "- Inference: given a query, split the embedding into M subspaces, generate a distance matrix d between query embedding subspaces and all centroids in each subspace. Iterate through each compressed item embeddings, find appropriate distance rows,columns that match cluster IDs of item embedding in each subspace. aggregate the partial distances from each subspace. Note: Some distance equations (ex: L2, inner product) are capable of aggragating from partial distances, not all (ex: cosine)\n",
    "- efficient memory, worst accuracy\n",
    "\n",
    "HNSW - Builds multilayer graph.\n",
    "- build: for each item embeddeing, randomly select in how many layers it should be represented in the graph. Insert top to bottom by starting from the entrypoint(s), defined by efConstruction, greedily find the nearest vertices at each layer and use them as entrypoints to the lower layers. Connect the item embedding to M vertices in layers where it is represented.\n",
    "- inference: given a query, starting from the top entrypoint(s), defined by efsearch, greedily find the nearest vertices at each layer and use them as entrypoints to the lower layers. select top k closest vertices to the query embedding at the bottom-most layer.\n",
    "- log(N) speed, great accuracy, not memory efficient\n",
    "\n",
    "\n",
    "In this use case, let's use HNSW+IVF+PQ and combine all the benefits. Build Kmean cluster using IVF, represent the IVF centroid in HNSW because we can search through the centroid in Log(N) inseat of O(N) in regular IVF, this allows us to make nlist bigger. Use PQ to comress actuall item embedding in each IVF cluster to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c13db5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tower import QueryTower, ItemTower\n",
    "import os\n",
    "\n",
    "\n",
    "query_input_size = user_embed_size + 2*item_embed_size + 2*artist_embed_size + 2*album_embed_size + 3\n",
    "candidate_input_size = item_embed_size + artist_embed_size + album_embed_size + 1\n",
    "\n",
    "# initialize and test query tower\n",
    "query_model = QueryTower(input_size = query_input_size,\n",
    "                         hidden_size = [1024, 512, 128],\n",
    "                         user_num_embeddings = len_unique_users+1,\n",
    "                         user_embed_size = user_embed_size,\n",
    "                         item_embed = item_embed,\n",
    "                         artist_embed = artist_embed,\n",
    "                         album_embed = album_embed,\n",
    "                         log_age_mean = 15.874020,\n",
    "                         log_age_std = 1.090574\n",
    "                        )\n",
    "\n",
    "\n",
    "# initialize and test item tower\n",
    "item_model = ItemTower(input_size = candidate_input_size,\n",
    "                       hidden_size = [1024, 512, 128],\n",
    "                       item_embed = item_embed,\n",
    "                       artist_embed = artist_embed,\n",
    "                       album_embed = album_embed\n",
    "                       )\n",
    "\n",
    "# load last checkpoint\n",
    "checkpoints = \"./checkpoints\"\n",
    "checkpoint = torch.load(os.path.join(checkpoints, f\"last_checkpoint.pth\"))\n",
    "query_model.load_state_dict(checkpoint[\"query_model_state_dict\"])\n",
    "item_model.load_state_dict(checkpoint[\"item_model_state_dict\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9635edde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded successfully\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import os\n",
    "\n",
    "filepath = '../../datasets/yambda/items'\n",
    "train_item_ds = load_from_disk(os.path.join(filepath, \"train\"))\n",
    "val_item_ds = load_from_disk(os.path.join(filepath, \"val\"))\n",
    "print(\"Datasets loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee918ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for the training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:16<00:00,  6.34it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "device = \"mps\"\n",
    "B = 8192\n",
    "item_model.to(device)\n",
    "item_model.eval()\n",
    "\n",
    "item_dataloader = DataLoader(train_item_ds.with_format(\"torch\"), batch_size=B)\n",
    "all_train_embeddings = []\n",
    "all_train_item_ids = []\n",
    "\n",
    "print(\"Generating embeddings for the training set...\")\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(item_dataloader):\n",
    "        \n",
    "        all_train_item_ids.append(batch[\"candidate_id\"].numpy())\n",
    "\n",
    "        # Move batch to the correct device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = item_model(batch)        \n",
    "        # Normalize embeddings (important for Faiss with inner product)\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        # Move to CPU and convert to numpy\n",
    "        all_train_embeddings.append(embeddings.cpu().numpy())\n",
    "\n",
    "train_embeddings_np = np.concatenate(all_train_embeddings, axis=0)\n",
    "train_item_id_np = np.concatenate(all_train_item_ids, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1004d69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.41it/s]\n"
     ]
    }
   ],
   "source": [
    "item_dataloader = DataLoader(val_item_ds.with_format(\"torch\"), batch_size=B)\n",
    "all_val_embeddings = []\n",
    "all_val_item_ids = []\n",
    "\n",
    "print(\"Generating embeddings for the validation set...\")\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(item_dataloader):\n",
    "\n",
    "        all_val_item_ids.append(batch[\"candidate_id\"].numpy())\n",
    "        # Move batch to the correct device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Generate embeddings\n",
    "        item_embeddings = item_model(batch)        \n",
    "        # Normalize embeddings (important for Faiss with inner product)\n",
    "        item_embeddings = torch.nn.functional.normalize(item_embeddings, p=2, dim=1)\n",
    "        \n",
    "        # Move to CPU and convert to numpy\n",
    "        all_val_embeddings.append(item_embeddings.cpu().numpy())\n",
    "\n",
    "val_embeddings_np = np.concatenate(all_val_embeddings, axis=0)\n",
    "val_item_id_np = np.concatenate(all_val_item_ids, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b3a0670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(862091, 128) (862091,)\n",
      "(15077, 128) (15077,)\n"
     ]
    }
   ],
   "source": [
    "print(train_embeddings_np.shape, train_item_id_np.shape)\n",
    "print(val_embeddings_np.shape, val_item_id_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d52ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import faiss\n",
    "\n",
    "# define index table metrics\n",
    "d = 128\n",
    "M = 32 # number of edges for each vertice in HNSW graph\n",
    "\n",
    "nlist = 1024 # number of Voronoi clusters (IVF)\n",
    "nsubspaces = 16 # number of subspaces to split each embedding (PQ), ideally divisible by 8\n",
    "nbits = 8 # 2^8 = 256 clusters, number of bits to encode cluster ID in each subspace\n",
    "metric = faiss.METRIC_INNER_PRODUCT\n",
    "# metric = faiss.METRIC_L2\n",
    "\n",
    "# quantizer = faiss.IndexHNSWFlat(d, M, metric) HNSW is an overkill if nlist is small\n",
    "quantizer = faiss.IndexFlatIP(d)\n",
    "index = faiss.IndexIVFPQ(quantizer, d, nlist, nsubspaces, nbits, metric)\n",
    "\n",
    "# Shuffle and take a subset for training, higher size simply crashes the kernel\n",
    "sample_size = 300_000\n",
    "train_sample = train_embeddings_np[np.random.choice(train_embeddings_np.shape[0], sample_size, replace=False)]\n",
    "\n",
    "index.train(train_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a996811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_embeddings = np.vstack([train_embeddings_np, val_embeddings_np]).astype('float32')\n",
    "all_ids = np.concatenate([train_item_id_np, val_item_id_np]).astype('int64')\n",
    "\n",
    "index.add_with_ids(all_embeddings, all_ids)\n",
    "\n",
    "# save index table\n",
    "faiss.write_index(index, \"items.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da19d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "two-tower (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
